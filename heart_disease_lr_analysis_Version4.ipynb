{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Heart Disease Risk Prediction with Logistic Regression (Homework)\n",
        "\n",
        "This notebook implements logistic regression from scratch (no scikit-learn) for the Heart Disease dataset (Kaggle/UCI). It respects and reuses what you've already done: data loading, target binarization, EDA, and outlier handling.\n",
        "\n",
        "Objectives:\n",
        "- Implement base functions (sigmoid, cost, gradients, gradient descent)\n",
        "- Train and evaluate the model (train/test metrics)\n",
        "- Visualize decision boundaries for feature pairs\n",
        "- Add L2 regularization and compare results\n",
        "\n",
        "Note about the dataset: the Kaggle file \"neurocipher/heartdisease\" contains 270 samples (not 303 as in the original UCI release). The target was already binarized to 1=Presence, 0=Absence in your previous work. We'll use those columns and keep original names."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load and Prepare the Dataset\n",
        "- Source: [Kaggle - neurocipher/heartdisease](https://www.kaggle.com/datasets/neurocipher/heartdisease)\n",
        "- Target binarization already done (`Heart Disease`: Presence→1, Absence→0)\n",
        "- EDA: summary, missing values, outliers (already completed)\n",
        "- Split: 70/30 stratified\n",
        "- Normalization: z-score on train; apply same parameters to test\n",
        "- Select ≥6 features: `Age`, `Cholesterol`, `BP`, `Max HR`, `ST depression`, `Number of vessels fluro`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports and configuration\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "np.random.seed(42)\n",
        "\n",
        "# Try to reuse 'df' if it exists from your previous work\n",
        "try:\n",
        "    df  # check reference\n",
        "    print(\"Using existing DataFrame ('df') from previous steps.\")\n",
        "except NameError:\n",
        "    print(\"'df' not found in the environment, attempting to download from Kaggle.\")\n",
        "    try:\n",
        "        import kagglehub\n",
        "        path = kagglehub.dataset_download(\"neurocipher/heartdisease\")\n",
        "        df = pd.read_csv(f\"{path}/Heart_Disease_Prediction.csv\")\n",
        "        print(\"Dataset downloaded from Kaggle and loaded.\")\n",
        "        # Binarize if not already\n",
        "        if df['Heart Disease'].dtype == object:\n",
        "            df['Heart Disease'] = df['Heart Disease'].map({'Presence': 1, 'Absence': 0})\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\"Could not load the dataset. Make sure you've executed the prior loading cell or have the CSV locally.\") from e\n",
        "\n",
        "# Show data summary\n",
        "print(\"Shape:\", df.shape)\n",
        "print(df[['Heart Disease']].head())\n",
        "\n",
        "target_col = 'Heart Disease'\n",
        "feature_cols = ['Age', 'Cholesterol', 'BP', 'Max HR', 'ST depression', 'Number of vessels fluro']\n",
        "\n",
        "# Basic checks\n",
        "assert target_col in df.columns, \"Column 'Heart Disease' not found in the DataFrame.\"\n",
        "for c in feature_cols:\n",
        "    assert c in df.columns, f\"Column '{c}' not found in the DataFrame.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Brief additional EDA (your main EDA was already done)\n",
        "print(\"\\nInfo:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\nDescription:\")\n",
        "display(df[feature_cols + [target_col]].describe())\n",
        "\n",
        "# Class distribution\n",
        "class_counts = df[target_col].value_counts()\n",
        "print(\"\\nClass distribution:\")\n",
        "print(class_counts)\n",
        "plt.figure(figsize=(4,3))\n",
        "sns.barplot(x=class_counts.index.astype(str), y=class_counts.values)\n",
        "plt.title(\"Class Distribution (Heart Disease)\")\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Note: Kaggle dataset used has 270 entries and ~44% presence (as per your prior EDA)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper functions: stratified split and normalization\n",
        "- Stratified 70/30 split by class\n",
        "- Standardization (z-score) on train and apply same parameters to test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def stratified_train_test_split(X, y, test_size=0.3, random_state=42):\n",
        "    \"\"\"Stratified split without scikit-learn.\n",
        "    Returns X_train, X_test, y_train, y_test\n",
        "    \"\"\"\n",
        "    np.random.seed(random_state)\n",
        "    classes = np.unique(y)\n",
        "    train_idx = []\n",
        "    test_idx = []\n",
        "    for cls in classes:\n",
        "        idx = np.where(y == cls)[0]\n",
        "        np.random.shuffle(idx)\n",
        "        n_test = int(len(idx) * test_size)\n",
        "        test_idx.extend(idx[:n_test])\n",
        "        train_idx.extend(idx[n_test:])\n",
        "    # optional final shuffle\n",
        "    train_idx = np.array(train_idx)\n",
        "    test_idx = np.array(test_idx)\n",
        "    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
        "\n",
        "def standardize_train_test(X_train, X_test):\n",
        "    \"\"\"Standardize X_train and apply the same parameters to X_test. Returns X_train_std, X_test_std, (mean, std).\"\"\"\n",
        "    mean = X_train.mean(axis=0)\n",
        "    std = X_train.std(axis=0)\n",
        "    # avoid division by 0\n",
        "    std[std == 0] = 1.0\n",
        "    X_train_std = (X_train - mean) / std\n",
        "    X_test_std = (X_test - mean) / std\n",
        "    return X_train_std, X_test_std, (mean, std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data (numpy matrices)\n",
        "X = df[feature_cols].values.astype(float)\n",
        "y = df[target_col].values.astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test = stratified_train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_train_std, X_test_std, scalers = standardize_train_test(X_train, X_test)\n",
        "\n",
        "print(\"Shapes:\", X_train_std.shape, X_test_std.shape, y_train.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Implement Logistic Regression (from scratch)\n",
        "- Functions: sigmoid, cost (binary cross-entropy), gradients, GD\n",
        "- Train on the full training set with α≈0.01, ≥1000 iterations\n",
        "- Plot cost vs. iterations\n",
        "- Predict and compute metrics on train/test (accuracy, precision, recall, F1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# From-scratch implementation\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def initialize_params(n_features):\n",
        "    w = np.zeros((n_features,))\n",
        "    b = 0.0\n",
        "    return w, b\n",
        "\n",
        "def predict_proba(X, w, b):\n",
        "    return sigmoid(X.dot(w) + b)\n",
        "\n",
        "def compute_cost_and_grads(X, y, w, b, reg_lambda=0.0):\n",
        "    m = X.shape[0]\n",
        "    y_hat = predict_proba(X, w, b)\n",
        "    # cost (cross-entropy): avoid log(0)\n",
        "    epsilon = 1e-8\n",
        "    cost = -(1/m) * (np.sum(y*np.log(y_hat+epsilon) + (1-y)*np.log(1-y_hat+epsilon)))\n",
        "    # L2 (do not regularize bias)\n",
        "    if reg_lambda > 0:\n",
        "        cost += (reg_lambda/(2*m)) * np.sum(w**2)\n",
        "    # gradients\n",
        "    dw = (1/m) * X.T.dot(y_hat - y)\n",
        "    db = (1/m) * np.sum(y_hat - y)\n",
        "    if reg_lambda > 0:\n",
        "        dw += (reg_lambda/m) * w\n",
        "    return cost, dw, db\n",
        "\n",
        "def gradient_descent(X, y, alpha=0.01, num_iters=2000, reg_lambda=0.0, verbose=False):\n",
        "    w, b = initialize_params(X.shape[1])\n",
        "    costs = []\n",
        "    for i in range(num_iters):\n",
        "        cost, dw, db = compute_cost_and_grads(X, y, w, b, reg_lambda)\n",
        "        w -= alpha * dw\n",
        "        b -= alpha * db\n",
        "        costs.append(cost)\n",
        "        if verbose and (i % 200 == 0 or i == num_iters-1):\n",
        "            print(f\"Iter {i}: cost={cost:.4f}\")\n",
        "    return w, b, costs\n",
        "\n",
        "def predict_labels(X, w, b, threshold=0.5):\n",
        "    probs = predict_proba(X, w, b)\n",
        "    return (probs >= threshold).astype(int)\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    return (y_true == y_pred).mean()\n",
        "\n",
        "def precision_recall_f1(y_true, y_pred):\n",
        "    # positive class = 1\n",
        "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
        "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
        "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
        "    precision = tp / (tp + fp + 1e-8)\n",
        "    recall = tp / (tp + fn + 1e-8)\n",
        "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
        "    return precision, recall, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train on the full training set without regularization (lambda=0)\n",
        "alpha = 0.01\n",
        "num_iters = 2000\n",
        "reg_lambda = 0.0\n",
        "\n",
        "w, b, costs = gradient_descent(X_train_std, y_train, alpha=alpha, num_iters=num_iters, reg_lambda=reg_lambda, verbose=True)\n",
        "\n",
        "# Plot cost vs iterations\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.plot(costs)\n",
        "plt.title(\"Cost vs Iterations (no regularization)\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Metrics on train and test\n",
        "y_train_pred = predict_labels(X_train_std, w, b)\n",
        "y_test_pred  = predict_labels(X_test_std,  w, b)\n",
        "\n",
        "acc_train = accuracy(y_train, y_train_pred)\n",
        "acc_test  = accuracy(y_test,  y_test_pred)\n",
        "prec_train, rec_train, f1_train = precision_recall_f1(y_train, y_train_pred)\n",
        "prec_test,  rec_test,  f1_test  = precision_recall_f1(y_test,  y_test_pred)\n",
        "\n",
        "metrics_df = pd.DataFrame({\n",
        "    'split': ['train', 'test'],\n",
        "    'accuracy': [acc_train, acc_test],\n",
        "    'precision': [prec_train, prec_test],\n",
        "    'recall': [rec_train, rec_test],\n",
        "    'f1': [f1_train, f1_test]\n",
        "})\n",
        "display(metrics_df)\n",
        "\n",
        "print(\"\\nCoefficients (w):\")\n",
        "for name, coef in zip(feature_cols, w):\n",
        "    print(f\"{name}: {coef:.4f}\")\n",
        "print(f\"Bias (b): {b:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comments on convergence and interpretation\n",
        "- The cost curve decreases with iterations, indicating learning.\n",
        "- The `w` coefficients show the effect (in logits) of each standardized feature on the presence probability.\n",
        "- Metrics: note differences between train/test and potential overfitting if test drops significantly relative to train."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Visualize Decision Boundaries (feature pairs)\n",
        "We train 2D models (no regularization) and plot the boundary: `w0*x0 + w1*x1 + b = 0` → a line in the plane of those two standardized features.\n",
        "\n",
        "Pairs (≥3):\n",
        "- Age vs Cholesterol\n",
        "- BP vs Max HR\n",
        "- ST depression vs Number of vessels fluro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pairs = [\n",
        "    ('Age', 'Cholesterol'),\n",
        "    ('BP', 'Max HR'),\n",
        "    ('ST depression', 'Number of vessels fluro')\n",
        "]\n",
        "\n",
        "def train_2d_and_plot(df, pair, target_col='Heart Disease', alpha=0.05, num_iters=3000):\n",
        "    f1, f2 = pair\n",
        "    # Extract matrices\n",
        "    X = df[[f1, f2]].values.astype(float)\n",
        "    y = df[target_col].values.astype(int)\n",
        "    # Stratified split\n",
        "    X_train, X_test, y_train, y_test = stratified_train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    # Standardize\n",
        "    X_train_std, X_test_std, scalers = standardize_train_test(X_train, X_test)\n",
        "    # Train\n",
        "    w, b, costs = gradient_descent(X_train_std, y_train, alpha=alpha, num_iters=num_iters, reg_lambda=0.0, verbose=False)\n",
        "    # Metrics\n",
        "    y_pred_test = predict_labels(X_test_std, w, b)\n",
        "    acc = accuracy(y_test, y_pred_test)\n",
        "    prec, rec, f1 = precision_recall_f1(y_test, y_pred_test)\n",
        "\n",
        "    # Plot: scatter + boundary\n",
        "    plt.figure(figsize=(5,4))\n",
        "    # points\n",
        "    plt.scatter(X_test_std[y_test==0][:,0], X_test_std[y_test==0][:,1], c='tab:blue', s=25, label='Absence (0)', alpha=0.7)\n",
        "    plt.scatter(X_test_std[y_test==1][:,0], X_test_std[y_test==1][:,1], c='tab:red', s=25, label='Presence (1)', alpha=0.7)\n",
        "    # boundary: w1*x + w2*y + b = 0 → y = -(w1/w2)x - b/w2\n",
        "    x_line = np.linspace(X_test_std[:,0].min()-0.5, X_test_std[:,0].max()+0.5, 100)\n",
        "    if abs(w[1]) > 1e-8:\n",
        "        y_line = -(w[0]/w[1])*x_line - b/w[1]\n",
        "        plt.plot(x_line, y_line, 'k--', label='Decision boundary')\n",
        "    plt.title(f\"Decision Boundary: {f1} vs {f2}\\nacc={acc:.2f}, prec={prec:.2f}, rec={rec:.2f}, f1={f1:.2f}\")\n",
        "    plt.xlabel(f1 + \" (std)\")\n",
        "    plt.ylabel(f2 + \" (std)\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return {\n",
        "        'pair': pair,\n",
        "        'w': w,\n",
        "        'b': b,\n",
        "        'acc': acc,\n",
        "        'prec': prec,\n",
        "        'rec': rec,\n",
        "        'f1': f1,\n",
        "        'cost_last': costs[-1]\n",
        "    }\n",
        "\n",
        "pair_results = []\n",
        "for p in pairs:\n",
        "    res = train_2d_and_plot(df, p, target_col=target_col, alpha=0.05, num_iters=3000)\n",
        "    pair_results.append(res)\n",
        "\n",
        "pd.DataFrame(pair_results)[['pair','acc','prec','rec','f1','cost_last']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comments by pair (separability and nonlinearity)\n",
        "- Age vs Cholesterol: often shows some linear separation when cholesterol is high.\n",
        "- BP vs Max HR: interesting combination; max HR tends to be informative.\n",
        "- ST depression vs Number of vessels fluro: frequently useful; vessel count and ST depression associate with presence.\n",
        "\n",
        "Observe the boundary shape: if the class mix does not appear linear, 2D logistic regression may not fully capture it (nonlinear features or more dimensions might be needed)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: L2 Regularization\n",
        "- Add L2 term to the cost and gradients: `cost += λ/(2m)||w||²`; `dw += (λ/m)w`\n",
        "- Try several λ: [0, 0.001, 0.01, 0.1, 1]\n",
        "- Retrain the full model and evaluate metrics and ||w||\n",
        "- Compare boundaries and costs for at least one pair (no reg vs reg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lambdas = [0.0, 0.001, 0.01, 0.1, 1.0]\n",
        "alpha = 0.01\n",
        "num_iters = 2000\n",
        "\n",
        "lambda_rows = []\n",
        "for lam in lambdas:\n",
        "    w_lam, b_lam, costs_lam = gradient_descent(X_train_std, y_train, alpha=alpha, num_iters=num_iters, reg_lambda=lam, verbose=False)\n",
        "    y_train_pred = predict_labels(X_train_std, w_lam, b_lam)\n",
        "    y_test_pred  = predict_labels(X_test_std,  w_lam, b_lam)\n",
        "    acc_train = accuracy(y_train, y_train_pred)\n",
        "    acc_test  = accuracy(y_test,  y_test_pred)\n",
        "    prec_train, rec_train, f1_train = precision_recall_f1(y_train, y_train_pred)\n",
        "    prec_test,  rec_test,  f1_test  = precision_recall_f1(y_test,  y_test_pred)\n",
        "    w_norm = np.linalg.norm(w_lam)\n",
        "    lambda_rows.append({\n",
        "        'lambda': lam,\n",
        "        'train_acc': acc_train,\n",
        "        'test_acc': acc_test,\n",
        "        'train_f1': f1_train,\n",
        "        'test_f1': f1_test,\n",
        "        'w_norm': w_norm,\n",
        "        'final_cost': costs_lam[-1]\n",
        "    })\n",
        "\n",
        "lambda_df = pd.DataFrame(lambda_rows)\n",
        "display(lambda_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visual comparison of costs and boundaries for one pair: Age vs Cholesterol (no reg vs reg)\n",
        "pair = ('Age', 'Cholesterol')\n",
        "f1, f2 = pair\n",
        "\n",
        "# Pair data\n",
        "X_pair = df[[f1, f2]].values.astype(float)\n",
        "y_pair = df[target_col].values.astype(int)\n",
        "X_pair_train, X_pair_test, y_pair_train, y_pair_test = stratified_train_test_split(X_pair, y_pair, test_size=0.3, random_state=42)\n",
        "X_pair_train_std, X_pair_test_std, pair_scalers = standardize_train_test(X_pair_train, X_pair_test)\n",
        "\n",
        "# Train without reg and with reg (λ=best from the table, or default 0.1)\n",
        "best_lambda = lambda_df.sort_values('test_f1', ascending=False)['lambda'].iloc[0]\n",
        "w0, b0, c0 = gradient_descent(X_pair_train_std, y_pair_train, alpha=0.05, num_iters=3000, reg_lambda=0.0)\n",
        "wR, bR, cR = gradient_descent(X_pair_train_std, y_pair_train, alpha=0.05, num_iters=3000, reg_lambda=best_lambda)\n",
        "\n",
        "plt.figure(figsize=(10,3))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(c0, label='no reg (λ=0)')\n",
        "plt.plot(cR, label=f'with reg (λ={best_lambda})')\n",
        "plt.title(\"Costs (Age vs Cholesterol)\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Cost\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.scatter(X_pair_test_std[y_pair_test==0][:,0], X_pair_test_std[y_pair_test==0][:,1], c='tab:blue', s=25, alpha=0.7, label='0')\n",
        "plt.scatter(X_pair_test_std[y_pair_test==1][:,0], X_pair_test_std[y_pair_test==1][:,1], c='tab:red', s=25, alpha=0.7, label='1')\n",
        "x_line = np.linspace(X_pair_test_std[:,0].min()-0.5, X_pair_test_std[:,0].max()+0.5, 100)\n",
        "if abs(w0[1]) > 1e-8:\n",
        "    y_line0 = -(w0[0]/w0[1])*x_line - b0/w0[1]\n",
        "    plt.plot(x_line, y_line0, 'k--', label='λ=0')\n",
        "if abs(wR[1]) > 1e-8:\n",
        "    y_lineR = -(wR[0]/wR[1])*x_line - bR/wR[1]\n",
        "    plt.plot(x_line, y_lineR, 'g--', label=f'λ={best_lambda}')\n",
        "plt.title(\"Decision Boundary (Age vs Cholesterol)\")\n",
        "plt.xlabel(\"Age (std)\")\n",
        "plt.ylabel(\"Cholesterol (std)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Compared metrics\n",
        "y0_pred = predict_labels(X_pair_test_std, w0, b0)\n",
        "yR_pred = predict_labels(X_pair_test_std, wR, bR)\n",
        "m0 = precision_recall_f1(y_pair_test, y0_pred)\n",
        "mR = precision_recall_f1(y_pair_test, yR_pred)\n",
        "print(f\"No reg (λ=0): acc={accuracy(y_pair_test, y0_pred):.3f}, prec={m0[0]:.3f}, rec={m0[1]:.3f}, f1={m0[2]:.3f}\")\n",
        "print(f\"With reg (λ={best_lambda}): acc={accuracy(y_pair_test, yR_pred):.3f}, prec={mR[0]:.3f}, rec={mR[1]:.3f}, f1={mR[2]:.3f}\")\n",
        "\n",
        "print(\"\\n||w|| no reg:\", np.linalg.norm(w0))\n",
        "print(\"||w|| with reg:\", np.linalg.norm(wR))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Regularization report\n",
        "- λ vs metrics table (train/test) and norm of w\n",
        "- With suitable L2 (e.g., λ≈0.01–0.1), generalization often improves (F1/accuracy on test), ||w|| decreases, and the cost stabilizes.\n",
        "\n",
        "Example conclusion: \"Optimal λ ≈ `best_lambda` improves test F1 vs. λ=0 by ~X% and reduces the norm of w, indicating less overfitting.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary (Markdown)\n",
        "- Download: Kaggle dataset \"neurocipher/heartdisease\" (270 samples), binary target (0/1). Distribution ~44% presence (per EDA).\n",
        "- Preprocessing: no missing; outliers already explored; selected 6 features; stratified 70/30 split; z-score normalization.\n",
        "- Training: logistic regression from scratch; cost decreases; train/test metrics reported.\n",
        "- Boundaries: 3 pairs plotted with decision lines; separability discussed.\n",
        "- L2 regularization: λ in [0, 0.001, 0.01, 0.1, 1]; observed trade-off and potential test improvement.\n",
        "\n",
        "Next steps (optional):\n",
        "- Pipeline and simulated deployment with Amazon SageMaker (not implemented here but you could package training in a container and a dummy endpoint).\n",
        "- Nonlinear feature engineering or variable interactions to improve boundaries."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
