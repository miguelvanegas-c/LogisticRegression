{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1085d82a",
   "metadata": {},
   "source": [
    "# Week 2 – Classification \n",
    "## Overfitting, Regularization, and Regularized Regression\n",
    "\n",
    "In this notebook we will:\n",
    "\n",
    "1. Understand **underfitting** and **overfitting**.\n",
    "2. Introduce **regularization** as a way to control model complexity.\n",
    "3. Define the **cost function with regularization** and its **derivatives**.\n",
    "4. Implement **regularized linear regression**.\n",
    "5. Implement **regularized logistic regression**.\n",
    "\n",
    "Notation (consistent with Hour 1):\n",
    "\n",
    "- Vectors (conceptual): $\\vec{w}, \\vec{x}$.\n",
    "- Linear regression hypothesis:  \n",
    "  $$\n",
    "  f_{\\vec{w}, b}(\\vec{x}) = \\vec{w} \\cdot \\vec{x} + b.\n",
    "  $$\n",
    "- Logistic regression hypothesis:  \n",
    "  $$\n",
    "  f_{\\vec{w}, b}(\\vec{x}) = \\sigma(\\vec{w} \\cdot \\vec{x} + b).\n",
    "  $$\n",
    "- For sample $i$:  \n",
    "  $$\n",
    "  f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)}) = f_{\\vec{w}, b}(\\vec{x}^{(i)}).\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4734e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 4)\n",
    "plt.rcParams[\"axes.grid\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62b2401",
   "metadata": {},
   "source": [
    "## 1. Underfitting and Overfitting\n",
    "\n",
    "Consider a simple **regression** problem: predict $y$ from $x$.  \n",
    "Suppose the true relationship is **nonlinear** (for example, a sine wave).\n",
    "\n",
    "We can choose different models:\n",
    "\n",
    "- **Underfitting**: model is too simple (e.g., a straight line).\n",
    "- **Good fit**: model captures the main pattern without memorizing noise.\n",
    "- **Overfitting**: model is too complex and memorizes noise/outliers.\n",
    "\n",
    "We will illustrate this using **polynomial regression**:\n",
    "\n",
    "- Degree 1: underfitting.\n",
    "- Degree 3: reasonable fit.\n",
    "- Degree 10: overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9826adcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic 1D regression data\n",
    "np.random.seed(42)\n",
    "m = 50\n",
    "X_scalar = np.linspace(-3, 3, m)\n",
    "y_true = np.sin(X_scalar)\n",
    "noise = 0.3 * np.random.randn(m)\n",
    "y = y_true + noise\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_scalar, y, label=\"data\")\n",
    "plt.plot(X_scalar, y_true, label=\"true function (sin)\")\n",
    "plt.title(\"Synthetic regression data\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8bd9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_poly_features(x, degree):\n",
    "    \"\"\"Create polynomial features [x, x^2, ..., x^degree] for 1D input x.\"\"\"\n",
    "    X_list = []\n",
    "    for d in range(1, degree + 1):\n",
    "        X_list.append(x ** d)\n",
    "    return np.vstack(X_list).T  # shape (m, degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c108aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear_regression_closed_form(X, y):\n",
    "    \"\"\"Fit linear regression using the normal equation (no regularization).\"\"\"\n",
    "    # Add a column of ones for the bias term\n",
    "    m = X.shape[0]\n",
    "    X_bias = np.column_stack([X, np.ones(m)])  # last column is for b\n",
    "    \n",
    "    # Solve [w; b] = (X^T X)^-1 X^T y\n",
    "    theta = np.linalg.pinv(X_bias.T @ X_bias) @ (X_bias.T @ y)\n",
    "    w = theta[:-1]\n",
    "    b = theta[-1]\n",
    "    return w, b\n",
    "\n",
    "\n",
    "def predict_linear(w, b, X):\n",
    "    return X @ w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0fda72",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = [1, 3, 10]\n",
    "x_plot = np.linspace(-3, 3, 200)\n",
    "y_true_plot = np.sin(x_plot)  # true function on dense grid\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "for i, deg in enumerate(degrees):\n",
    "    X_poly = make_poly_features(X_scalar, deg)\n",
    "    w, b = fit_linear_regression_closed_form(X_poly, y)\n",
    "    \n",
    "    X_plot_poly = make_poly_features(x_plot, deg)\n",
    "    y_plot = predict_linear(w, b, X_plot_poly)\n",
    "    \n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.scatter(X_scalar, y, label=\"data\", s=15)\n",
    "    plt.plot(x_plot, y_true_plot, label=\"true (sin)\")\n",
    "    plt.plot(x_plot, y_plot, label=f\"degree {deg} fit\")\n",
    "    plt.title(f\"Polynomial degree {deg}\")\n",
    "    plt.xlabel(\"x\")\n",
    "    if i == 0:\n",
    "        plt.ylabel(\"y\")\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b59ba27",
   "metadata": {},
   "source": [
    "In the plots:\n",
    "\n",
    "- **Degree 1** (left): the model is too simple → it **underfits** the data.  \n",
    "- **Degree 3** (middle): the model follows the main trend → **good generalization**.  \n",
    "- **Degree 10** (right): the model wiggles and follows noise → it **overfits**.\n",
    "\n",
    "**Goal:** Control model complexity to avoid overfitting.  \n",
    "One key tool: **regularization**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9f7fd8",
   "metadata": {},
   "source": [
    "## 2. Regularization\n",
    "\n",
    "Idea: penalize **large parameters** so the model prefers **simpler** solutions.\n",
    "\n",
    "For a parameter vector $\\vec{w} = (w_1, \\dots, w_n)$, we define the **L2 regularization term**:\n",
    "\n",
    "$$\n",
    "R(\\vec{w}) = \\sum_{j=1}^n w_j^2.\n",
    "$$\n",
    "\n",
    "We **do not** include the bias $b$ in this penalty (only the weights).\n",
    "\n",
    "Regularization adds this penalty to the original cost function, controlled by a\n",
    "hyperparameter $\\lambda \\ge 0$:\n",
    "\n",
    "- $\\lambda = 0$: no regularization (risk of overfitting).\n",
    "- Large $\\lambda$: strong penalty (risk of underfitting).\n",
    "- Intermediate $\\lambda$: good compromise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97a1eb1",
   "metadata": {},
   "source": [
    "## 3. Cost Function with Regularization\n",
    "\n",
    "### 3.1 Regularized Linear Regression\n",
    "\n",
    "For linear regression we had (without regularization):\n",
    "\n",
    "$$\n",
    "J(\\vec{w}, b) =\n",
    "\\frac{1}{2m}\\sum_{i=1}^m\n",
    "\\left(\n",
    "f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)}) - y^{(i)}\n",
    "\\right)^2,\n",
    "$$\n",
    "\n",
    "with\n",
    "$$\n",
    "f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)}) = \\vec{w}\\cdot\\vec{x}^{(i)} + b.\n",
    "$$\n",
    "\n",
    "We now add the **L2 regularization term**:\n",
    "\n",
    "$$\n",
    "J_{\\text{reg}}(\\vec{w}, b) =\n",
    "\\frac{1}{2m}\\sum_{i=1}^m\n",
    "\\left(\n",
    "f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)}) - y^{(i)}\n",
    "\\right)^2\n",
    "+\n",
    "\\frac{\\lambda}{2m}\\sum_{j=1}^n w_j^2.\n",
    "$$\n",
    "\n",
    "Note: the sum over $j$ runs only over the components of $\\vec{w}$, not $b$.\n",
    "\n",
    "### 3.2 Regularized Logistic Regression\n",
    "\n",
    "For logistic regression, the **unregularized** cost was:\n",
    "\n",
    "$$\n",
    "J(\\vec{w}, b) =\n",
    "-\\frac{1}{m}\n",
    "\\sum_{i=1}^m\n",
    "\\left[\n",
    "y^{(i)} \\log f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)}) \\;+\\;\n",
    "(1 - y^{(i)}) \\log\\big(1 - f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)})\\big)\n",
    "\\right],\n",
    "$$\n",
    "\n",
    "with\n",
    "$$\n",
    "f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)}) = \\sigma(\\vec{w}\\cdot\\vec{x}^{(i)} + b).\n",
    "$$\n",
    "\n",
    "The **regularized** cost is:\n",
    "\n",
    "$$\n",
    "J_{\\text{reg}}(\\vec{w}, b) =\n",
    "-\\frac{1}{m}\n",
    "\\sum_{i=1}^m\n",
    "\\left[\n",
    "y^{(i)} \\log f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)}) \\;+\\;\n",
    "(1 - y^{(i)}) \\log\\big(1 - f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)})\\big)\n",
    "\\right]\n",
    "+\n",
    "\\frac{\\lambda}{2m}\\sum_{j=1}^n w_j^2.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9763f44c",
   "metadata": {},
   "source": [
    "### 3.3 Gradients for Regularized Linear Regression\n",
    "\n",
    "Recall:\n",
    "$$\n",
    "J_{\\text{reg}}(\\vec{w}, b) =\n",
    "\\frac{1}{2m}\\sum_{i=1}^m\n",
    "\\left(\n",
    "f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)}) - y^{(i)}\n",
    "\\right)^2\n",
    "+\n",
    "\\frac{\\lambda}{2m}\\sum_{j=1}^n w_j^2,\n",
    "$$\n",
    "with\n",
    "$$\n",
    "f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)}) = \\vec{w}\\cdot\\vec{x}^{(i)} + b.\n",
    "$$\n",
    "\n",
    "Define the error for each example:\n",
    "$$\n",
    "e^{(i)} = f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)}) - y^{(i)}.\n",
    "$$\n",
    "\n",
    "Then, for each weight $w_j$:\n",
    "$$\n",
    "\\frac{\\partial J_{\\text{reg}}}{\\partial w_j}\n",
    "=\n",
    "\\frac{1}{m}\\sum_{i=1}^m e^{(i)} x^{(i)}_j\n",
    "+\n",
    "\\frac{\\lambda}{m} w_j,\n",
    "$$\n",
    "\n",
    "and for the bias:\n",
    "$$\n",
    "\\frac{\\partial J_{\\text{reg}}}{\\partial b}\n",
    "=\n",
    "\\frac{1}{m}\\sum_{i=1}^m e^{(i)}.\n",
    "$$\n",
    "\n",
    "These are the derivatives used in **gradient descent**:\n",
    "$$\n",
    "w_j := w_j - \\alpha \\frac{\\partial J_{\\text{reg}}}{\\partial w_j},\n",
    "\\quad\n",
    "b := b - \\alpha \\frac{\\partial J_{\\text{reg}}}{\\partial b}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a58afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_lin_reg_reg(w, b, X, y, lam):\n",
    "    \"\"\"Regularized linear regression cost J_reg(w, b).\"\"\"\n",
    "    m, n = X.shape\n",
    "    preds = X @ w + b  # f_{w,b}^{(i)}(x^{(i)})\n",
    "    errors = preds - y\n",
    "    mse_term = (1 / (2 * m)) * np.sum(errors ** 2)\n",
    "    reg_term = (lam / (2 * m)) * np.sum(w ** 2)\n",
    "    return mse_term + reg_term\n",
    "\n",
    "\n",
    "def compute_gradient_lin_reg_reg(w, b, X, y, lam):\n",
    "    \"\"\"Gradient of regularized linear regression cost w.r.t w and b.\"\"\"\n",
    "    m, n = X.shape\n",
    "    preds = X @ w + b\n",
    "    errors = preds - y\n",
    "    \n",
    "    dj_dw = (1 / m) * (X.T @ errors) + (lam / m) * w\n",
    "    dj_db = (1 / m) * np.sum(errors)\n",
    "    return dj_dw, dj_db\n",
    "\n",
    "\n",
    "def gradient_descent_lin_reg_reg(X, y, w_init, b_init, alpha, lam, num_iters):\n",
    "    w = w_init.copy()\n",
    "    b = b_init\n",
    "    J_history = []\n",
    "    for i in range(num_iters):\n",
    "        dj_dw, dj_db = compute_gradient_lin_reg_reg(w, b, X, y, lam)\n",
    "        w -= alpha * dj_dw\n",
    "        b -= alpha * dj_db\n",
    "        J_history.append(compute_cost_lin_reg_reg(w, b, X, y, lam))\n",
    "    return w, b, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecb583b",
   "metadata": {},
   "source": [
    "For **regularized linear regression**, there is a **closed-form analytical solution**, known as **ridge regression**. \n",
    "\n",
    "Compared to gradient descent, the ridge closed-form solution has sveral advantages:\n",
    "\n",
    "- Is **exact** (for this cost function): we obtain the global minimizer directly.\n",
    "- Has **no learning rate** to tune.\n",
    "- Is **numerically stable** for polynomial features of moderate degree.\n",
    "- Produces **predictable, clean plots**:\n",
    "  - $\\lambda = 0$: strong overfitting (wiggly curve),\n",
    "  - Intermediate $\\lambda$: controlled complexity,\n",
    "  - Large $\\lambda$: underfitting (almost linear).\n",
    "\n",
    "For our polynomial regression demo, this is ideal:  \n",
    "students can clearly see the effect of **the same degree, different $\\lambda$** on the shape of the curve.\n",
    "\n",
    "The ridge closed-form method also has limitations:\n",
    "\n",
    "- It requires computing the inverse (or pseudo-inverse) of a $(n+1) \\times (n+1)$ matrix.\n",
    "  - This is fine for small or moderate $n$,\n",
    "  - But can be expensive when $n$ is very large.\n",
    "- It only exists for **linear models** with quadratic costs.\n",
    "  - There is no closed-form solution for logistic regression,\n",
    "  - Nor for neural networks and most deep learning models.\n",
    "\n",
    "So, while perfect for our **regularized linear regression demo**, it is **not a general replacement** for gradient descent.\n",
    "\n",
    "The code above shows the implementation of gradient descent with regularization. However, for the demo we use the ridge regression method defined below. You can find a detailed explanation in the apendix [here.](./APENDIX-RidgeVsGradientDescentInRegularizedLinearRegression.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c5bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear_regression_ridge(X, y, lam):\n",
    "    \"\"\"\n",
    "    Closed-form ridge regression:\n",
    "    minimize (1/(2m)) ||Xw + b - y||^2 + (lam/(2m)) ||w||^2\n",
    "    We do NOT regularize the bias term.\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    # Add bias column\n",
    "    X_bias = np.column_stack([X, np.ones(m)])  # shape (m, n+1)\n",
    "\n",
    "    # Identity for regularization (do not regularize bias -> last entry = 0)\n",
    "    I = np.eye(n + 1)\n",
    "    I[-1, -1] = 0.0\n",
    "\n",
    "    # Ridge solution: theta = (X^T X + lam * I)^(-1) X^T y\n",
    "    theta = np.linalg.pinv(X_bias.T @ X_bias + lam * I) @ (X_bias.T @ y)\n",
    "    w = theta[:-1]\n",
    "    b = theta[-1]\n",
    "    return w, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533e1b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compare fits for degree 10 polynomial with different lambda values (closed-form ridge)\n",
    "\n",
    "degree = 10\n",
    "X_poly_high = make_poly_features(X_scalar, degree)  # features built on original x\n",
    "x_plot = np.linspace(-3, 3, 200)\n",
    "X_plot_poly_high = make_poly_features(x_plot, degree)\n",
    "y_true_plot = np.sin(x_plot)\n",
    "\n",
    "lambdas = [0.0, 0.1, 10.0]\n",
    "colors = [\"tab:blue\", \"tab:orange\", \"tab:green\"]\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_scalar, y, label=\"data\", s=20)\n",
    "plt.plot(x_plot, y_true_plot, \"k-\", label=\"true (sin)\")\n",
    "\n",
    "for lam, col in zip(lambdas, colors):\n",
    "    w_reg, b_reg = fit_linear_regression_ridge(X_poly_high, y, lam)\n",
    "    y_plot_reg = X_plot_poly_high @ w_reg + b_reg\n",
    "    plt.plot(x_plot, y_plot_reg, color=col, label=f\"lambda={lam}\")\n",
    "\n",
    "plt.title(\"Degree 10 polynomial with different regularization strengths\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe752961",
   "metadata": {},
   "source": [
    "In the plot above (degree 10 polynomial):\n",
    "\n",
    "- **$\\lambda = 0$**: no regularization → the curve **overfits** strongly.\n",
    "- **Small $\\lambda$ (e.g., 0.1)**: the curve is smoother and follows the main trend.\n",
    "- **Large $\\lambda$ (e.g., 10)**: the curve becomes almost linear or too simple → risk of **underfitting**.\n",
    "\n",
    "Regularization **shrinks** the parameters $w_j$ toward zero and controls complexity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76316300",
   "metadata": {},
   "source": [
    "## 4. Regularized Logistic Regression\n",
    "\n",
    "We now implement **regularized logistic regression**.\n",
    "\n",
    "Recall the (unregularized) hypothesis:\n",
    "$$\n",
    "f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)}) = \\sigma(\\vec{w} \\cdot \\vec{x}^{(i)} + b).\n",
    "$$\n",
    "\n",
    "Regularized cost:\n",
    "$$\n",
    "J_{\\text{reg}}(\\vec{w}, b) =\n",
    "-\\frac{1}{m}\n",
    "\\sum_{i=1}^m\n",
    "\\left[\n",
    "y^{(i)} \\log f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)}) \\;+\\;\n",
    "(1 - y^{(i)}) \\log\\big(1 - f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)})\\big)\n",
    "\\right]\n",
    "+\n",
    "\\frac{\\lambda}{2m}\\sum_{j=1}^n w_j^2.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f6051c",
   "metadata": {},
   "source": [
    "### 4.1 Gradients for Regularized Logistic Regression\n",
    "\n",
    "For each example:\n",
    "$$\n",
    "f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)}) = \\sigma(\\vec{w}\\cdot\\vec{x}^{(i)} + b).\n",
    "$$\n",
    "\n",
    "Define the error:\n",
    "$$\n",
    "e^{(i)} = f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)}) - y^{(i)}.\n",
    "$$\n",
    "\n",
    "Then the gradients of the regularized cost are:\n",
    "\n",
    "For each weight $w_j$:\n",
    "$$\n",
    "\\frac{\\partial J_{\\text{reg}}}{\\partial w_j}\n",
    "=\n",
    "\\frac{1}{m}\\sum_{i=1}^m e^{(i)} x^{(i)}_j\n",
    "+\n",
    "\\frac{\\lambda}{m} w_j,\n",
    "$$\n",
    "\n",
    "and for the bias:\n",
    "$$\n",
    "\\frac{\\partial J_{\\text{reg}}}{\\partial b}\n",
    "=\n",
    "\\frac{1}{m}\\sum_{i=1}^m e^{(i)}.\n",
    "$$\n",
    "\n",
    "Gradient descent updates are:\n",
    "\n",
    "$$\n",
    "w_j := w_j - \\alpha \\frac{\\partial J_{\\text{reg}}}{\\partial w_j},\n",
    "\\quad\n",
    "b := b - \\alpha \\frac{\\partial J_{\\text{reg}}}{\\partial b}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e811b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def compute_cost_log_reg_reg(w, b, X, y, lam):\n",
    "    \"\"\"Regularized logistic regression cost.\"\"\"\n",
    "    m, n = X.shape\n",
    "    z = X @ w + b\n",
    "    f = sigmoid(z)  # f_{w,b}^{(i)}(x^{(i)})\n",
    "    eps = 1e-8\n",
    "    f_clipped = np.clip(f, eps, 1 - eps)\n",
    "    # Cross-entropy term\n",
    "    ce = - (1 / m) * np.sum(\n",
    "        y * np.log(f_clipped) + (1 - y) * np.log(1 - f_clipped)\n",
    "    )\n",
    "    # Regularization term\n",
    "    reg = (lam / (2 * m)) * np.sum(w ** 2)\n",
    "    return ce + reg\n",
    "\n",
    "\n",
    "def compute_gradient_log_reg_reg(w, b, X, y, lam):\n",
    "    \"\"\"Gradient of regularized logistic regression cost.\"\"\"\n",
    "    m, n = X.shape\n",
    "    z = X @ w + b\n",
    "    f = sigmoid(z)\n",
    "    error = f - y  # f_{w,b}^{(i)}(x^{(i)}) - y^{(i)}\n",
    "    dj_dw = (1 / m) * (X.T @ error) + (lam / m) * w\n",
    "    dj_db = (1 / m) * np.sum(error)\n",
    "    return dj_dw, dj_db\n",
    "\n",
    "\n",
    "def gradient_descent_log_reg_reg(X, y, w_init, b_init, alpha, lam, num_iters):\n",
    "    w = w_init.copy()\n",
    "    b = b_init\n",
    "    J_history = []\n",
    "    for i in range(num_iters):\n",
    "        dj_dw, dj_db = compute_gradient_log_reg_reg(w, b, X, y, lam)\n",
    "        w -= alpha * dj_dw\n",
    "        b -= alpha * dj_db\n",
    "        J_history.append(compute_cost_log_reg_reg(w, b, X, y, lam))\n",
    "    return w, b, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e800a95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple 2D classification dataset (two clusters)\n",
    "np.random.seed(0)\n",
    "m_class = 200\n",
    "mean0 = np.array([-1.0, -1.0])\n",
    "mean1 = np.array([1.0, 1.0])\n",
    "cov = 0.3 * np.eye(2)\n",
    "\n",
    "X0 = np.random.multivariate_normal(mean0, cov, m_class // 2)\n",
    "X1 = np.random.multivariate_normal(mean1, cov, m_class // 2)\n",
    "X_class = np.vstack([X0, X1])\n",
    "y_class = np.hstack([np.zeros(m_class // 2), np.ones(m_class // 2)])\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_class[y_class == 0, 0], X_class[y_class == 0, 1], label=\"y = 0\")\n",
    "plt.scatter(X_class[y_class == 1, 0], X_class[y_class == 1, 1], label=\"y = 1\")\n",
    "plt.title(\"2D classification dataset\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494d2a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary_logreg(w, b, X, y, title):\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    z = grid @ w + b\n",
    "    probs = sigmoid(z).reshape(xx.shape)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.contourf(xx, yy, probs, levels=[0, 0.5, 1], alpha=0.2, colors=[\"tab:blue\", \"tab:orange\"])\n",
    "    plt.scatter(X[y == 0, 0], X[y == 0, 1], label=\"y = 0\")\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], label=\"y = 1\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Compare no regularization vs regularization\n",
    "lambdas = [0.0, 10.0]\n",
    "for lam in lambdas:\n",
    "    w0 = np.zeros(2)\n",
    "    b0 = 0.0\n",
    "    w_reg, b_reg, J_hist = gradient_descent_log_reg_reg(\n",
    "        X_class, y_class, w0, b0, alpha=0.5, lam=lam, num_iters=300\n",
    "    )\n",
    "    plot_decision_boundary_logreg(\n",
    "        w_reg, b_reg, X_class, y_class, title=f\"Regularized Logistic Regression (lambda={lam})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff4d2a",
   "metadata": {},
   "source": [
    "## Summary (Hour 2)\n",
    "\n",
    "In this notebook we:\n",
    "\n",
    "1. Visualized **underfitting** and **overfitting** using polynomial regression.\n",
    "2. Introduced **L2 regularization** to penalize large weights:\n",
    "   $$\n",
    "   R(\\vec{w}) = \\sum_{j=1}^n w_j^2.\n",
    "   $$\n",
    "3. Defined **regularized cost functions** and their **derivatives**:\n",
    "   - For linear regression:\n",
    "     $$\n",
    "     J_{\\text{reg}}(\\vec{w}, b) =\n",
    "     \\frac{1}{2m}\\sum_{i=1}^m\n",
    "     \\left(\n",
    "     f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)}) - y^{(i)}\n",
    "     \\right)^2\n",
    "     + \\frac{\\lambda}{2m}\\sum_{j=1}^n w_j^2,\n",
    "     $$\n",
    "     $$\n",
    "     \\frac{\\partial J_{\\text{reg}}}{\\partial w_j}\n",
    "     =\n",
    "     \\frac{1}{m}\\sum_{i=1}^m e^{(i)} x^{(i)}_j\n",
    "     +\n",
    "     \\frac{\\lambda}{m} w_j,\n",
    "     \\quad\n",
    "     \\frac{\\partial J_{\\text{reg}}}{\\partial b}\n",
    "     =\n",
    "     \\frac{1}{m}\\sum_{i=1}^m e^{(i)}.\n",
    "     $$\n",
    "   - For logistic regression:\n",
    "     $$\n",
    "     J_{\\text{reg}}(\\vec{w}, b) =\n",
    "     -\\frac{1}{m}\n",
    "     \\sum_{i=1}^m\n",
    "     \\left[\n",
    "     y^{(i)} \\log f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)}) \\;+\\;\n",
    "     (1 - y^{(i)}) \\log\\big(1 - f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)})\\big)\n",
    "     \\right]\n",
    "     +\n",
    "     \\frac{\\lambda}{2m}\\sum_{j=1}^n w_j^2,\n",
    "     $$\n",
    "     with analogous gradients:\n",
    "     $$\n",
    "     \\frac{\\partial J_{\\text{reg}}}{\\partial w_j}\n",
    "     =\n",
    "     \\frac{1}{m}\\sum_{i=1}^m e^{(i)} x^{(i)}_j\n",
    "     +\n",
    "     \\frac{\\lambda}{m} w_j,\n",
    "     \\quad\n",
    "     \\frac{\\partial J_{\\text{reg}}}{\\partial b}\n",
    "     =\n",
    "     \\frac{1}{m}\\sum_{i=1}^m e^{(i)}.\n",
    "     $$\n",
    "4. Implemented **regularized linear regression** and saw how $\\lambda$ affects the fit of a high-degree polynomial.\n",
    "5. Implemented **regularized logistic regression** and visualized decision boundaries with and without regularization.\n",
    "\n",
    "These derivations make explicit how **gradient descent** is applied when we include\n",
    "regularization in both linear and logistic regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41939f7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
