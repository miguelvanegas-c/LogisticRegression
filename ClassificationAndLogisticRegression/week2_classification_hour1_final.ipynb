{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2b05f1e",
   "metadata": {},
   "source": [
    "# Week 2 – Classification\n",
    "## Logistic Regression: From Intuition to Gradient Descent\n",
    "\n",
    "In this notebook we will:\n",
    "\n",
    "1. Motivate **classification** and compare it with linear regression.\n",
    "2. Introduce the **logistic regression model**:\n",
    "   $$\n",
    "   f_{\\vec{w}, b}(\\vec{x}) = \\sigma(\\vec{w} \\cdot \\vec{x} + b)\n",
    "   $$\n",
    "3. Understand **decision boundaries**:\n",
    "   - Linear boundaries in the original feature space.\n",
    "   - Nonlinear boundaries via **feature transformations** (polynomial features).\n",
    "4. Define the **logistic loss** and the **cost function**.\n",
    "5. Derive and implement the **gradients** of the cost function.\n",
    "6. Implement **gradient descent** and visualize **cost vs iterations**.\n",
    "\n",
    "Notation (we keep it explicit and consistent):\n",
    "\n",
    "- Vectors: $\\vec{w}, \\vec{x}$ (conceptual). In code they are NumPy arrays.\n",
    "- Hypothesis (model): $f_{\\vec{w}, b}(\\vec{x})$.\n",
    "- For sample $i$: $f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)}) = f_{\\vec{w}, b}(\\vec{x}^{(i)})$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed122d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy matplotlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40325a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 4)\n",
    "plt.rcParams[\"axes.grid\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac229e7",
   "metadata": {},
   "source": [
    "## 1. Motivation: Regression vs Classification\n",
    "\n",
    "In **linear regression**, the goal is to predict a **real-valued output**:\n",
    "$$\n",
    "\\hat{y} \\in \\mathbb{R}\n",
    "$$\n",
    "\n",
    "Examples:\n",
    "- Predicting house prices.\n",
    "- Predicting temperature.\n",
    "\n",
    "In **binary classification**, the goal is to predict a **class label**:\n",
    "$$\n",
    "\\hat{y} \\in \\{0, 1\\}\n",
    "$$\n",
    "\n",
    "Examples:\n",
    "- Email: spam ($y = 1$) vs not spam ($y = 0$).\n",
    "- Transaction: fraudulent ($y = 1$) vs normal ($y = 0$).\n",
    "- Tumor: malignant ($y = 1$) vs benign ($y = 0$).\n",
    "\n",
    "Instead of predicting a raw number, we want a **probability** that the class is 1:\n",
    "$$\n",
    "f_{\\vec{w}, b}(\\vec{x}) = P(y = 1 \\mid \\vec{x}; \\vec{w}, b)\n",
    "$$\n",
    "\n",
    "Then we can decide:\n",
    "- Predict $\\hat{y} = 1$ if $f_{\\vec{w}, b}(\\vec{x}) \\ge 0.5$.\n",
    "- Predict $\\hat{y} = 0$ otherwise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4e3b3b",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression Model\n",
    "\n",
    "We define the **hypothesis** (model) as:\n",
    "$$\n",
    "f_{\\vec{w}, b}(\\vec{x}) = \\sigma(z)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "z = \\vec{w} \\cdot \\vec{x} + b\n",
    "$$\n",
    "\n",
    "The function $\\sigma(z)$ is the **sigmoid**:\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Interpretation:\n",
    "$$\n",
    "f_{\\vec{w}, b}(\\vec{x}) = P(y = 1 \\mid \\vec{x}; \\vec{w}, b)\n",
    "$$\n",
    "\n",
    "So for sample $i$:\n",
    "$$\n",
    "f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)}) = \\sigma(\\vec{w} \\cdot \\vec{x}^{(i)} + b)\n",
    "$$\n",
    "\n",
    "We will first visualize the sigmoid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc6f040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Compute the sigmoid of z.\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "# Visualize the sigmoid function\n",
    "z_values = np.linspace(-10, 10, 200)\n",
    "sig_values = sigmoid(z_values)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(z_values, sig_values)\n",
    "plt.title(\"Sigmoid function\")\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"sigma(z)\")\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.axhline(0.5, color=\"red\", linestyle=\"--\", linewidth=0.8)\n",
    "plt.axvline(0, color=\"black\", linewidth=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbc08f4",
   "metadata": {},
   "source": [
    "The plot shows $\\sigma(z)$:\n",
    "\n",
    "- For large negative $z$, $\\sigma(z) \\approx 0$.\n",
    "- For large positive $z$, $\\sigma(z) \\approx 1$.\n",
    "- At $z = 0$, $\\sigma(0) = 0.5$.\n",
    "\n",
    "This shape is ideal to model **probabilities**:\n",
    "\n",
    "- If $z$ is very negative, $f_{\\vec{w}, b}(\\vec{x})$ is close to 0 (model is confident that $y = 0$).\n",
    "- If $z$ is very positive, $f_{\\vec{w}, b}(\\vec{x})$ is close to 1 (model is confident that $y = 1$).\n",
    "- Around $z = 0$, $f_{\\vec{w}, b}(\\vec{x}) \\approx 0.5$ (model is uncertain).\n",
    "\n",
    "Next, we connect this to **decision boundaries**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3341433",
   "metadata": {},
   "source": [
    "## 3. Decision Boundary – Linear Case\n",
    "\n",
    "Recall the hypothesis:\n",
    "$$\n",
    "f_{\\vec{w}, b}(\\vec{x}) = \\sigma(\\vec{w} \\cdot \\vec{x} + b).\n",
    "$$\n",
    "\n",
    "Our prediction rule is:\n",
    "$$\n",
    "\\hat{y} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } f_{\\vec{w}, b}(\\vec{x}) \\ge 0.5 \\\\\n",
    "0 & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Because $\\sigma(z) \\ge 0.5$ if and only if $z \\ge 0$, we have:\n",
    "$$\n",
    "f_{\\vec{w}, b}(\\vec{x}) \\ge 0.5\n",
    "\\iff\n",
    "\\vec{w} \\cdot \\vec{x} + b \\ge 0.\n",
    "$$\n",
    "\n",
    "The **decision boundary** is defined by the points where the model is exactly at probability 0.5:\n",
    "$$\n",
    "\\vec{w} \\cdot \\vec{x} + b = 0.\n",
    "$$\n",
    "\n",
    "- In 2D ($x_1, x_2$), this is a **line**.\n",
    "- In 3D, it is a **plane**.\n",
    "- In general, it is a **hyperplane**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aa5e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a simple linearly separable 2D dataset\n",
    "np.random.seed(0)\n",
    "\n",
    "m = 100  # number of examples\n",
    "X_linear = np.random.randn(m, 2)\n",
    "\n",
    "# Underlying \"true\" boundary: x2 = x1 + 0.5\n",
    "# Label 1 if above the line, 0 if below\n",
    "y_linear = (X_linear[:, 1] > X_linear[:, 0] + 0.5).astype(int)\n",
    "\n",
    "\n",
    "def plot_linear_data_with_boundary(w, b, X, y, title):\n",
    "    \"\"\"Plot 2D data (X, y) and a linear decision boundary defined by w, b.\"\"\"\n",
    "    plt.figure()\n",
    "    plt.scatter(X[y == 0, 0], X[y == 0, 1], marker=\"o\", label=\"y = 0\")\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], marker=\"x\", label=\"y = 1\")\n",
    "\n",
    "    # Decision boundary: w0*x1 + w1*x2 + b = 0  => x2 = -(w0*x1 + b) / w1\n",
    "    x1_vals = np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 100)\n",
    "    if abs(w[1]) > 1e-6:\n",
    "        x2_vals = -(w[0] * x1_vals + b) / w[1]\n",
    "        plt.plot(x1_vals, x2_vals, \"k--\", label=\"decision boundary\")\n",
    "    else:\n",
    "        x1_line = -b / w[0]\n",
    "        plt.axvline(x1_line, color=\"k\", linestyle=\"--\", label=\"decision boundary\")\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example boundary (not trained yet)\n",
    "w_example = np.array([1.0, -1.0])\n",
    "b_example = 0.0\n",
    "\n",
    "plot_linear_data_with_boundary(w_example, b_example, X_linear, y_linear,\n",
    "                               \"Linearly separable data and a linear boundary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444e409e",
   "metadata": {},
   "source": [
    "## 3.1 Nonlinear Boundaries via Feature Transformations\n",
    "\n",
    "With the **original features** $x_1, x_2$, a logistic regression model\n",
    "$$\n",
    "f_{\\vec{w}, b}(\\vec{x}) = \\sigma(\\vec{w} \\cdot \\vec{x} + b)\n",
    "$$\n",
    "can only represent **linear** decision boundaries in the $(x_1, x_2)$ plane.\n",
    "\n",
    "However, if we transform the input using a **feature mapping**:\n",
    "$$\n",
    "\\vec{\\phi}(\\vec{x}) =\n",
    "[x_1,\\, x_2,\\, x_1^2,\\, x_2^2,\\, x_1 x_2],\n",
    "$$\n",
    "and define the hypothesis in the new feature space as\n",
    "$$\n",
    "f_{\\vec{w}, b}(\\vec{x}) = \\sigma(\\vec{w} \\cdot \\vec{\\phi}(\\vec{x}) + b),\n",
    "$$\n",
    "then the decision boundary is\n",
    "$$\n",
    "\\vec{w} \\cdot \\vec{\\phi}(\\vec{x}) + b = 0,\n",
    "$$\n",
    "which is **linear in the feature space** of $\\vec{\\phi}(\\vec{x})$, but can appear\n",
    "**nonlinear in the original $(x_1, x_2)$ space** (e.g., circles, ellipses).\n",
    "\n",
    "We now create a **circular dataset** to illustrate a nonlinear boundary (we will not train\n",
    "the nonlinear model yet in this first hour, only set up the example).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f68bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a circular dataset\n",
    "np.random.seed(1)\n",
    "\n",
    "m_circ = 400\n",
    "X_circ = np.random.randn(m_circ, 2)\n",
    "r = np.sqrt(X_circ[:, 0] ** 2 + X_circ[:, 1] ** 2)\n",
    "\n",
    "# Label 1 if inside circle of radius ~1, else 0\n",
    "y_circ = (r < 1.0).astype(int)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_circ[y_circ == 0, 0], X_circ[y_circ == 0, 1], label=\"y = 0\")\n",
    "plt.scatter(X_circ[y_circ == 1, 0], X_circ[y_circ == 1, 1], label=\"y = 1\")\n",
    "plt.title(\"Circular dataset (nonlinear boundary in original space)\")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def polynomial_features(X):\n",
    "    \"\"\"Simple polynomial feature mapping for 2D inputs.\n",
    "    \n",
    "    phi(x1, x2) = [x1, x2, x1^2, x2^2, x1*x2]\n",
    "    \"\"\"\n",
    "    x1 = X[:, 0]\n",
    "    x2 = X[:, 1]\n",
    "    X_poly = np.column_stack([\n",
    "        x1,\n",
    "        x2,\n",
    "        x1**2,\n",
    "        x2**2,\n",
    "        x1 * x2\n",
    "    ])\n",
    "    return X_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ac53c1",
   "metadata": {},
   "source": [
    "## 4. Cost Function for Logistic Regression\n",
    "\n",
    "We want to choose $\\vec{w}, b$ so that the model's predictions\n",
    "$$\n",
    "f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)}) = f_{\\vec{w}, b}(\\vec{x}^{(i)})\n",
    "$$\n",
    "are close to the true labels $y^{(i)}$.\n",
    "\n",
    "For **logistic regression**, we use the **logistic loss** per example:\n",
    "$$\n",
    "\\ell^{(i)} =\n",
    "\\begin{cases}\n",
    "-\\log\\left(f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)})\\right) & \\text{if } y^{(i)} = 1, \\\\\n",
    "-\\log\\left(1 - f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)})\\right) & \\text{if } y^{(i)} = 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Intuition:\n",
    "\n",
    "- If $y^{(i)} = 1$, we want $f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)})$ close to 1.  \n",
    "  Then $-\\log(\\cdot)$ is small.\n",
    "- If $y^{(i)} = 0$, we want $f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)})$ close to 0.  \n",
    "  Then $-\\log(1 - \\cdot)$ is small.\n",
    "- **Confident wrong predictions** produce a very large loss.\n",
    "\n",
    "The overall cost function is the average loss over all $m$ examples:\n",
    "$$\n",
    "J(\\vec{w}, b) =\n",
    "-\\frac{1}{m}\n",
    "\\sum_{i=1}^m\n",
    "\\left[\n",
    "y^{(i)} \\log f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)})\n",
    "+\n",
    "(1 - y^{(i)}) \\log\\big(1 - f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)})\\big)\n",
    "\\right].\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10606074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(w, b, X, y):\n",
    "    \"\"\"Compute logistic regression cost J(w, b) using explicit model.\"\"\"\n",
    "    m, n = X.shape\n",
    "\n",
    "    # For each example i: f_{w,b}^{(i)}(x^{(i)}) = sigmoid(w^T x^{(i)} + b)\n",
    "    z = X @ w + b\n",
    "    f = sigmoid(z)\n",
    "\n",
    "    # To avoid log(0), clip probabilities\n",
    "    eps = 1e-8\n",
    "    f_clipped = np.clip(f, eps, 1 - eps)\n",
    "\n",
    "    J = -(1 / m) * np.sum(\n",
    "        y * np.log(f_clipped) + (1 - y) * np.log(1 - f_clipped)\n",
    "    )\n",
    "    return J\n",
    "\n",
    "\n",
    "# Quick test on linear dataset with zero parameters\n",
    "w_test = np.zeros(2)\n",
    "b_test = 0.0\n",
    "J_test = compute_cost(w_test, b_test, X_linear, y_linear)\n",
    "J_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a88f3ec",
   "metadata": {},
   "source": [
    "## 5. Gradients of the Cost Function\n",
    "\n",
    "We want to minimize $J(\\vec{w}, b)$ with respect to $\\vec{w}$ and $b$.  \n",
    "We use **gradient descent**, which requires the partial derivatives.\n",
    "\n",
    "For each parameter $w_j$:\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_j}\n",
    "=\n",
    "\\frac{1}{m}\n",
    "\\sum_{i=1}^m\n",
    "\\left(\n",
    "f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)}) - y^{(i)}\n",
    "\\right)\n",
    "x^{(i)}_j,\n",
    "$$\n",
    "and for the bias:\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b}\n",
    "=\n",
    "\\frac{1}{m}\n",
    "\\sum_{i=1}^m\n",
    "\\left(\n",
    "f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)}) - y^{(i)}\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "Vectorized view (conceptual):\n",
    "\n",
    "1. Compute all $z^{(i)} = \\vec{w} \\cdot \\vec{x}^{(i)} + b$ and\n",
    "   $$\n",
    "   f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)}) = \\sigma(z^{(i)}).\n",
    "   $$\n",
    "2. Define the error vector $\\vec{e}$ with components\n",
    "   $$\n",
    "   e^{(i)} = f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)}) - y^{(i)}.\n",
    "   $$\n",
    "3. Then\n",
    "   $$\n",
    "   \\nabla_{\\vec{w}} J = \\frac{1}{m} X^\\top \\vec{e}, \\quad\n",
    "   \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m e^{(i)}.\n",
    "   $$\n",
    "\n",
    "We now implement this in code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd243c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(w, b, X, y):\n",
    "    \"\"\"Compute gradients dJ/dw and dJ/db with explicit model.\"\"\"\n",
    "    m, n = X.shape\n",
    "\n",
    "    # f_{w,b}^{(i)}(x^{(i)})\n",
    "    z = X @ w + b\n",
    "    f = sigmoid(z)\n",
    "\n",
    "    error = f - y  # e^{(i)} = f_{w,b}^{(i)}(x^{(i)}) - y^{(i)}\n",
    "\n",
    "    dj_dw = (1 / m) * (X.T @ error)\n",
    "    dj_db = (1 / m) * np.sum(error)\n",
    "\n",
    "    return dj_dw, dj_db\n",
    "\n",
    "\n",
    "# Quick test\n",
    "dj_dw_test, dj_db_test = compute_gradient(w_test, b_test, X_linear, y_linear)\n",
    "dj_dw_test, dj_db_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34000137",
   "metadata": {},
   "source": [
    "## 6. Gradient Descent\n",
    "\n",
    "To minimize $J(\\vec{w}, b)$, we update the parameters in the opposite direction\n",
    "of the gradient.\n",
    "\n",
    "For each weight $w_j$:\n",
    "$$\n",
    "w_j := w_j - \\alpha \\frac{\\partial J}{\\partial w_j},\n",
    "$$\n",
    "and for the bias:\n",
    "$$\n",
    "b := b - \\alpha \\frac{\\partial J}{\\partial b},\n",
    "$$\n",
    "\n",
    "where $\\alpha > 0$ is the **learning rate**.\n",
    "\n",
    "Algorithm (high level):\n",
    "\n",
    "1. Initialize $\\vec{w}$ and $b$ (e.g., zeros).\n",
    "2. Repeat for a number of iterations:\n",
    "   - Compute $\\frac{\\partial J}{\\partial w_j}$ and $\\frac{\\partial J}{\\partial b}$.\n",
    "   - Update $\\vec{w}$ and $b$ with the rules above.\n",
    "3. Track $J(\\vec{w}, b)$ to see if it decreases.\n",
    "\n",
    "We now implement gradient descent and train our model on the linear dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad723f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_init, b_init, alpha, num_iters, print_every=0):\n",
    "    \"\"\"Run gradient descent to optimize w and b.\"\"\"\n",
    "    w = w_init.copy()\n",
    "    b = b_init\n",
    "    J_history = []\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        dj_dw, dj_db = compute_gradient(w, b, X, y)\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "        J = compute_cost(w, b, X, y)\n",
    "        J_history.append(J)\n",
    "\n",
    "        if print_every > 0 and (i % print_every == 0 or i == num_iters - 1):\n",
    "            print(f\"Iteration {i:4d}: J(w, b) = {J:.4f}\")\n",
    "\n",
    "    return w, b, J_history\n",
    "\n",
    "\n",
    "# Train on the linear dataset\n",
    "m_lin, n_lin = X_linear.shape\n",
    "w0 = np.zeros(n_lin)\n",
    "b0 = 0.0\n",
    "\n",
    "alpha = 0.5\n",
    "num_iters = 200\n",
    "\n",
    "w_trained, b_trained, J_hist = gradient_descent(\n",
    "    X_linear, y_linear, w0, b0, alpha, num_iters, print_every=50\n",
    ")\n",
    "\n",
    "w_trained, b_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dec458",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(J_hist)\n",
    "plt.title(\"Cost vs iterations (gradient descent)\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"J(w, b)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c42c267",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_linear_data_with_boundary(\n",
    "    w_trained, b_trained, X_linear, y_linear,\n",
    "    \"Linear dataset with learned decision boundary\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db289965",
   "metadata": {},
   "source": [
    "## 7. Summary (Hour 1)\n",
    "\n",
    "In this first hour we have:\n",
    "\n",
    "1. **Motivation**  \n",
    "   - Moved from predicting real values (regression) to predicting classes (classification).\n",
    "   - Defined the model output as a probability:\n",
    "     $$\n",
    "     f_{\\vec{w}, b}(\\vec{x}) = P(y = 1 \\mid \\vec{x}; \\vec{w}, b).\n",
    "     $$\n",
    "\n",
    "2. **Logistic regression model**  \n",
    "   - Hypothesis:\n",
    "     $$\n",
    "     f_{\\vec{w}, b}(\\vec{x}) = \\sigma(\\vec{w} \\cdot \\vec{x} + b).\n",
    "     $$\n",
    "   - Sigmoid $\\sigma(z)$ maps any real number to the interval $(0, 1)$.\n",
    "   - For each example:\n",
    "     $$\n",
    "     f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)}) = f_{\\vec{w}, b}(\\vec{x}^{(i)}).\n",
    "     $$\n",
    "\n",
    "3. **Decision boundary**  \n",
    "   - Prediction rule:\n",
    "     $$\n",
    "     f_{\\vec{w}, b}(\\vec{x}) \\ge 0.5\n",
    "     \\iff\n",
    "     \\vec{w} \\cdot \\vec{x} + b \\ge 0.\n",
    "     $$\n",
    "   - Decision boundary is given by\n",
    "     $$\n",
    "     \\vec{w} \\cdot \\vec{x} + b = 0,\n",
    "     $$\n",
    "     which is linear in the original features.\n",
    "   - With feature maps $\\vec{\\phi}(\\vec{x})$, logistic regression can represent\n",
    "     **nonlinear boundaries** in the original input space.\n",
    "\n",
    "4. **Cost function**  \n",
    "   - Per-example logistic loss:\n",
    "     $$\n",
    "     \\ell^{(i)} =\n",
    "     -\\left[\n",
    "     y^{(i)} \\log f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)})\n",
    "     +\n",
    "     (1 - y^{(i)}) \\log\\left(1 - f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)})\\right)\n",
    "     \\right].\n",
    "     $$\n",
    "   - Overall cost:\n",
    "     $$\n",
    "     J(\\vec{w}, b) =\n",
    "     -\\frac{1}{m}\n",
    "     \\sum_{i=1}^m\n",
    "     \\left[\n",
    "     y^{(i)} \\log f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)})\n",
    "     +\n",
    "     (1 - y^{(i)}) \\log\\left(1 - f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)})\\right)\n",
    "     \\right].\n",
    "     $$\n",
    "\n",
    "5. **Gradients and gradient descent**  \n",
    "   - Gradients:\n",
    "     $$\n",
    "     \\frac{\\partial J}{\\partial w_j}\n",
    "     =\n",
    "     \\frac{1}{m}\n",
    "     \\sum_{i=1}^m\n",
    "     \\left(\n",
    "     f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)}) - y^{(i)}\n",
    "     \\right) x_j^{(i)},\n",
    "     $$\n",
    "     $$\n",
    "     \\frac{\\partial J}{\\partial b}\n",
    "     =\n",
    "     \\frac{1}{m}\n",
    "     \\sum_{i=1}^m\n",
    "     \\left(\n",
    "     f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)}) - y^{(i)}\n",
    "     \\right).\n",
    "     $$\n",
    "   - Gradient descent updates:\n",
    "     $$\n",
    "     w_j := w_j - \\alpha \\frac{\\partial J}{\\partial w_j}, \\quad\n",
    "     b := b - \\alpha \\frac{\\partial J}{\\partial b}.\n",
    "     $$\n",
    "   - We implemented gradient descent, observed the **cost decreasing**, and visualized\n",
    "     the **learned decision boundary** on a 2D dataset.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
