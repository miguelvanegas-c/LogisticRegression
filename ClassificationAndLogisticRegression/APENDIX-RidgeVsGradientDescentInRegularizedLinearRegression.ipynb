{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfb77fa8",
   "metadata": {},
   "source": [
    "# APENDIX: Ridge Closed-Form vs. Gradient Descent in Regularized Linear Regression\n",
    "\n",
    "In this section we compare two ways of training **regularized linear regression**:\n",
    "\n",
    "1. **Gradient descent** (iterative optimization).\n",
    "2. **Ridge closed-form solution** (analytical solution).\n",
    "\n",
    "Both aim to minimize the *same* regularized cost function, but they behave quite differently in practice, especially when we use **high-degree polynomial features** (e.g., degree 10) as in our BootCamp example.\n",
    "\n",
    "We will:\n",
    "\n",
    "- Explain why gradient descent sometimes misbehaves in this setting.\n",
    "- Motivate why the ridge closed-form solution is a good **alternative algorithm** for this specific demo.\n",
    "- Clarify when to prefer each approach.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Problem Setting\n",
    "\n",
    "We consider **regularized linear regression** with parameters $\\vec{w} \\in \\mathbb{R}^n$ and bias $b \\in \\mathbb{R}$.\n",
    "\n",
    "For a feature vector $\\vec{x}^{(i)} \\in \\mathbb{R}^n$, the hypothesis is\n",
    "\n",
    "$$\n",
    "f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)}) = f_{\\vec{w}, b}(\\vec{x}^{(i)}) \n",
    "= \\vec{w}\\cdot\\vec{x}^{(i)} + b.\n",
    "$$\n",
    "\n",
    "The **regularized cost function** is\n",
    "\n",
    "$$\n",
    "J_{\\text{reg}}(\\vec{w}, b)\n",
    "=\n",
    "\\frac{1}{2m}\n",
    "\\sum_{i=1}^m\n",
    "\\left(\n",
    "f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)}) - y^{(i)}\n",
    "\\right)^2\n",
    "+\n",
    "\\frac{\\lambda}{2m}\n",
    "\\sum_{j=1}^n w_j^2,\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $m$: number of training examples.\n",
    "- $\\lambda \\ge 0$: regularization strength.\n",
    "- The sum over $j$ includes only the components of $\\vec{w}$ (we do **not** regularize $b$).\n",
    "\n",
    "We often obtain features via **polynomial expansion**, for example in 1D:\n",
    "\n",
    "$$\n",
    "\\vec{x}^{(i)} = \n",
    "\\big[x^{(i)}, (x^{(i)})^2, \\dots, (x^{(i)})^{10}\\big]\n",
    "$$\n",
    "\n",
    "for degree-10 polynomial regression.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Gradient Descent: General but Sensitive\n",
    "\n",
    "### 2.1 Gradient Descent Updates\n",
    "\n",
    "We first derive the derivatives used in **gradient descent**.\n",
    "\n",
    "Define the error for each example:\n",
    "\n",
    "$$\n",
    "e^{(i)} = f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)}) - y^{(i)}.\n",
    "$$\n",
    "\n",
    "Then the gradients of the regularized cost are:\n",
    "\n",
    "For each weight $w_j$,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J_{\\text{reg}}}{\\partial w_j}\n",
    "=\n",
    "\\frac{1}{m}\n",
    "\\sum_{i=1}^m e^{(i)} x_j^{(i)}\n",
    "+\n",
    "\\frac{\\lambda}{m} w_j,\n",
    "$$\n",
    "\n",
    "and for the bias,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J_{\\text{reg}}}{\\partial b}\n",
    "=\n",
    "\\frac{1}{m}\n",
    "\\sum_{i=1}^m e^{(i)}.\n",
    "$$\n",
    "\n",
    "Gradient descent updates the parameters as:\n",
    "\n",
    "$$\n",
    "w_j := w_j - \\alpha \\frac{\\partial J_{\\text{reg}}}{\\partial w_j},\n",
    "\\quad\n",
    "b := b - \\alpha \\frac{\\partial J_{\\text{reg}}}{\\partial b},\n",
    "$$\n",
    "\n",
    "where $\\alpha > 0$ is the **learning rate**.\n",
    "\n",
    "### 2.2 Why Gradient Descent Is Important\n",
    "\n",
    "Gradient descent is central in machine learning because:\n",
    "\n",
    "- It works for **any differentiable** cost function.\n",
    "- It scales to very **large datasets**.\n",
    "- It is the foundation for training more complex models:\n",
    "  - Logistic regression,\n",
    "  - Neural networks,\n",
    "  - Deep learning architectures.\n",
    "\n",
    "In our BootCamp, we use gradient descent to:\n",
    "\n",
    "- Make students compute and understand **derivatives**,\n",
    "- Show how the model is updated **iteratively**,\n",
    "- Connect the math with actual **code**.\n",
    "\n",
    "### 2.3 Problems with High-Degree Polynomial Features\n",
    "\n",
    "However, when we use **high-degree polynomials**, gradient descent can become **numerically unstable**:\n",
    "\n",
    "1. **Large feature values**\n",
    "\n",
    "   If $x \\in [-3, 3]$, then\n",
    "\n",
    "   $$\n",
    "   x^{10} \\approx 3^{10} = 59\\,049,\n",
    "   $$\n",
    "\n",
    "   so some components of $\\vec{x}^{(i)}$ are huge.\n",
    "\n",
    "   This leads to:\n",
    "\n",
    "   - Very large predictions $f_{\\vec{w}, b}^{(i)}(\\vec{x}^{(i)})$,\n",
    "   - Very large errors $e^{(i)}$,\n",
    "   - Very large squared errors $(e^{(i)})^2$,\n",
    "   - Overflow warnings like `RuntimeWarning: overflow encountered in square`.\n",
    "\n",
    "2. **Exploding gradients**\n",
    "\n",
    "   Because the cost and errors are large, gradients also become large:\n",
    "\n",
    "   - Parameter updates become huge,\n",
    "   - The algorithm “jumps” instead of gradually converging,\n",
    "   - The cost can diverge instead of decreasing.\n",
    "\n",
    "3. **Sensitivity to hyperparameters**\n",
    "\n",
    "   To stabilize gradient descent we often need to:\n",
    "\n",
    "   - Carefully **scale features**,\n",
    "   - Use very small learning rates $\\alpha$,\n",
    "   - Increase the number of iterations,\n",
    "   - Tune $\\alpha$ by trial and error.\n",
    "\n",
    "   Even then, a slight misconfiguration can result in:\n",
    "\n",
    "   - Very noisy or wrong curves,\n",
    "   - Poor visualization of overfitting vs regularization.\n",
    "\n",
    "### 2.4 Consequence for the Classroom Demo\n",
    "\n",
    "For the **conceptual demo** we want:\n",
    "\n",
    "- Degree 10, $\\lambda = 0$: **strong overfit**, very wiggly curve.\n",
    "- Degree 10, small $\\lambda$: **smooth but flexible**, good fit.\n",
    "- Degree 10, large $\\lambda$: **very smooth**, underfit almost like a line.\n",
    "\n",
    "Gradient descent with high-degree polynomial features:\n",
    "\n",
    "- May fail to converge,\n",
    "- May produce all three curves looking similar and underfit,\n",
    "- Or even produce numerical errors and NaNs.\n",
    "\n",
    "This makes it hard to show students the **clean, expected behavior** of regularization.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Ridge Closed-Form: Exact and Stable Alternative\n",
    "\n",
    "For **regularized linear regression**, there is a **closed-form analytical solution**, known as **ridge regression**.\n",
    "\n",
    "### 3.1 Ridge Closed-Form Solution\n",
    "\n",
    "Let $X \\in \\mathbb{R}^{m \\times n}$ be the design matrix of features (without bias), and $y \\in \\mathbb{R}^m$ the vector of targets.\n",
    "\n",
    "We construct $X_{\\text{bias}} \\in \\mathbb{R}^{m \\times (n+1)}$ by adding a column of ones for the bias:\n",
    "\n",
    "$$\n",
    "X_{\\text{bias}} = [X \\;|\\; \\mathbf{1}],\n",
    "$$\n",
    "\n",
    "and define\n",
    "\n",
    "$$\n",
    "\\theta =\n",
    "\\begin{bmatrix}\n",
    "\\vec{w} \\\\\n",
    "b\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{n+1}.\n",
    "$$\n",
    "\n",
    "Then the **ridge solution** is\n",
    "\n",
    "$$\n",
    "\\theta\n",
    "=\n",
    "\\left(\n",
    "X_{\\text{bias}}^\\top X_{\\text{bias}} + \\lambda I_{\\text{reg}}\n",
    "\\right)^{-1}\n",
    "X_{\\text{bias}}^\\top y,\n",
    "$$\n",
    "\n",
    "where $I_{\\text{reg}}$ is the identity matrix of size $(n+1) \\times (n+1)$ but with the last diagonal entry set to 0, so we **do not regularize the bias**:\n",
    "\n",
    "- For $j = 1, \\dots, n$: we regularize $w_j$,\n",
    "- For the last entry (bias $b$): no regularization.\n",
    "\n",
    "This gives us $\\vec{w}$ and $b$ **in one shot**, without iterations.\n",
    "\n",
    "### 3.2 Advantages of the Ridge Closed-Form Solution\n",
    "\n",
    "Compared to gradient descent, the ridge closed-form solution:\n",
    "\n",
    "- Is **exact** (for this cost function): we obtain the global minimizer directly.\n",
    "- Has **no learning rate** to tune.\n",
    "- Is **numerically stable** for polynomial features of moderate degree.\n",
    "- Produces **predictable, clean plots**:\n",
    "  - $\\lambda = 0$: strong overfitting (wiggly curve),\n",
    "  - Intermediate $\\lambda$: controlled complexity,\n",
    "  - Large $\\lambda$: underfitting (almost linear).\n",
    "\n",
    "For our polynomial regression demo, this is ideal:  \n",
    "students can clearly see the effect of **the same degree, different $\\lambda$** on the shape of the curve.\n",
    "\n",
    "### 3.3 Limitations of the Closed-Form Solution\n",
    "\n",
    "The ridge closed-form method also has limitations:\n",
    "\n",
    "- It requires computing the inverse (or pseudo-inverse) of a $(n+1) \\times (n+1)$ matrix.\n",
    "  - This is fine for small or moderate $n$,\n",
    "  - But can be expensive when $n$ is very large.\n",
    "- It only exists for **linear models** with quadratic costs.\n",
    "  - There is no closed-form solution for logistic regression,\n",
    "  - Nor for neural networks and most deep learning models.\n",
    "\n",
    "So, while perfect for our **regularized linear regression demo**, it is **not a general replacement** for gradient descent.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Why Use an Alternative Algorithm (Ridge) to Gradient Descent Here?\n",
    "\n",
    "Putting it all together:\n",
    "\n",
    "- For teaching **derivatives** and **algorithmic thinking**, we use:\n",
    "  - Gradient descent,\n",
    "  - Explicit expressions for $\\frac{\\partial J_{\\text{reg}}}{\\partial w_j}$ and $\\frac{\\partial J_{\\text{reg}}}{\\partial b}$,\n",
    "  - Simple models (low-degree or well-scaled features).\n",
    "\n",
    "- For teaching the **effect of regularization** on a **high-degree polynomial model**, we prefer:\n",
    "  - The ridge closed-form solution,\n",
    "  - Because it is more **robust**, **stable**, and **does not require tuning**,\n",
    "  - And it gives the **expected visual behavior** (overfit vs underfit) in a clean way.\n",
    "\n",
    "In short:\n",
    "\n",
    "- **Gradient descent** is the **general-purpose learning algorithm** you will use everywhere.\n",
    "- **Ridge closed-form** is a **specialized, more reliable tool** for this particular kind of model, perfect for showing how $\\lambda$ controls model complexity.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Takeaways for Students\n",
    "\n",
    "1. Gradient descent and ridge closed-form are two ways of solving the **same optimization problem** for regularized linear regression.\n",
    "2. Gradient descent:\n",
    "   - Is **general** and used in many models,\n",
    "   - But can be **numerically fragile** in high-degree polynomial settings.\n",
    "3. Ridge closed-form:\n",
    "   - Is **exact** and **stable** for this specific case,\n",
    "   - Makes the effect of **regularization** visually clear and reliable.\n",
    "4. In real practice:\n",
    "   - Use closed-form when the model is simple and the feature dimension is moderate,\n",
    "   - Use gradient descent (or variants like SGD, Adam) for large-scale or more complex models.\n",
    "\n",
    "This is why, in our BootCamp materials, we keep **gradient descent** to teach the learning algorithm, but we use the **ridge closed-form** for the key **visualization of regularization** in high-degree polynomial regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a083d0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
